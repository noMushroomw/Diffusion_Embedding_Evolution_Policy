{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d059e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Embedding Decoder Network\n",
    "class EmbeddingDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, action_dim=None, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Network that decodes embeddings back to actions\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of the embedding vectors\n",
    "            action_dim: Dimension of the action space\n",
    "            hidden_dim: Dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super(EmbeddingDecoder, self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "        \"\"\"\n",
    "        Decode an embedding into an action\n",
    "        \n",
    "        Args:\n",
    "            embedding: The embedding vector to decode [batch_size, embedding_dim]\n",
    "            \n",
    "        Returns:\n",
    "            action: The decoded action [batch_size, action_dim]\n",
    "        \"\"\"\n",
    "        return self.decoder(embedding)\n",
    "\n",
    "\n",
    "# Diffusion Model Components\n",
    "class Diffusion:\n",
    "    def __init__(self, embedding_dim, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        \"\"\"\n",
    "        Diffusion process for generating embeddings\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of embeddings\n",
    "            timesteps: Number of diffusion timesteps\n",
    "            beta_start: Starting noise schedule\n",
    "            beta_end: Ending noise schedule\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.timesteps = timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        # Define noise schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
    "        \n",
    "        # Calculations for diffusion q(x_t | x_{t-1})\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "    \n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion process: q(x_t | x_0)\n",
    "        \n",
    "        Args:\n",
    "            x_0: Initial embedding [batch_size, embedding_dim]\n",
    "            t: Timestep [batch_size]\n",
    "            noise: Optional pre-generated noise [batch_size, embedding_dim]\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Noisy embedding at timestep t\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "            \n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].reshape(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    def p_sample(self, model, x_t, t, state, t_index):\n",
    "        \"\"\"\n",
    "        Sample from p(x_{t-1} | x_t) using model prediction\n",
    "        \n",
    "        Args:\n",
    "            model: Denoising model\n",
    "            x_t: Embedding at timestep t [batch_size, embedding_dim]\n",
    "            t: Current timestep [batch_size]\n",
    "            state: Current state input [batch_size, state_dim]\n",
    "            t_index: Index of current timestep (integer)\n",
    "            \n",
    "        Returns:\n",
    "            x_{t-1}: Denoised embedding at timestep t-1\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            betas_t = self.betas[t].reshape(-1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1)\n",
    "            sqrt_recip_alphas_t = self.sqrt_recip_alphas[t].reshape(-1, 1)\n",
    "            \n",
    "            # Predict noise\n",
    "            predicted_noise = model(x_t, t, state)\n",
    "            \n",
    "            # Compute mean for p(x_{t-1} | x_t)\n",
    "            model_mean = sqrt_recip_alphas_t * (\n",
    "                x_t - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t\n",
    "            )\n",
    "            \n",
    "            if t_index == 0:\n",
    "                return model_mean\n",
    "            else:\n",
    "                posterior_variance_t = self.posterior_variance[t].reshape(-1, 1)\n",
    "                noise = torch.randn_like(x_t)\n",
    "                return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "    \n",
    "    def p_sample_loop(self, model, shape, state):\n",
    "        \"\"\"\n",
    "        Generate samples using the diffusion model\n",
    "        \n",
    "        Args:\n",
    "            model: Denoising model\n",
    "            shape: Shape of embeddings to generate [batch_size, embedding_dim]\n",
    "            state: Current state input [batch_size, state_dim]\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: Generated embeddings [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        batch_size = shape[0]\n",
    "        \n",
    "        # Start from pure noise\n",
    "        embeddings = torch.randn(shape).to(device)\n",
    "        \n",
    "        # Iteratively denoise\n",
    "        for i in tqdm(reversed(range(0, self.timesteps)), desc='Sampling', total=self.timesteps):\n",
    "            t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n",
    "            embeddings = self.p_sample(model, embeddings, t, state, i)\n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "    def train_loss(self, model, x_0, state):\n",
    "        \"\"\"\n",
    "        Compute training loss for diffusion model\n",
    "        \n",
    "        Args:\n",
    "            model: Denoising model\n",
    "            x_0: Original embeddings [batch_size, embedding_dim]\n",
    "            state: Current state input [batch_size, state_dim]\n",
    "            \n",
    "        Returns:\n",
    "            loss: Mean squared error between predicted and actual noise\n",
    "        \"\"\"\n",
    "        batch_size = x_0.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t = self.q_sample(x_0, t, noise)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(x_t, t, state)\n",
    "        \n",
    "        # Compute loss\n",
    "        return F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "\n",
    "# Denoising Model (UNet-like architecture for diffusion)\n",
    "class ConditionalDenoisingModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, state_dim, time_dim=128, hidden_dim=256):\n",
    "        \"\"\"\n",
    "        Denoising model for diffusion process\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of the embedding vectors\n",
    "            state_dim: Dimension of the state space\n",
    "            time_dim: Dimension of time embeddings\n",
    "            hidden_dim: Dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super(ConditionalDenoisingModel, self).__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        \n",
    "        # State encoder\n",
    "        self.state_encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Conditional UNet-like architecture\n",
    "        # Down blocks\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + hidden_dim + time_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//2)\n",
    "        )\n",
    "        \n",
    "        # Middle block\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//2)\n",
    "        )\n",
    "        \n",
    "        # Up blocks with skip connections\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim//2 + hidden_dim//2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, state):\n",
    "        \"\"\"\n",
    "        Predict noise in embedding at timestep t\n",
    "        \n",
    "        Args:\n",
    "            x: Noisy embedding at timestep t [batch_size, embedding_dim]\n",
    "            t: Current timestep [batch_size]\n",
    "            state: Current state input [batch_size, state_dim]\n",
    "            \n",
    "        Returns:\n",
    "            predicted_noise: Predicted noise in the embedding [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Encode time step\n",
    "        t_emb = self.time_mlp(t)\n",
    "        \n",
    "        # Encode state\n",
    "        s_emb = self.state_encoder(state)\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        x_input = torch.cat([x, s_emb, t_emb], dim=1)\n",
    "        \n",
    "        # Down path\n",
    "        down1 = self.down1(x_input)\n",
    "        down2 = self.down2(down1)\n",
    "        \n",
    "        # Middle\n",
    "        mid = self.mid(down2)\n",
    "        \n",
    "        # Up path with skip connections\n",
    "        up1 = self.up1(torch.cat([mid, down2], dim=1))\n",
    "        up2 = self.up2(torch.cat([up1, down1], dim=1))\n",
    "        \n",
    "        return up2\n",
    "\n",
    "\n",
    "# Sinusoidal position embeddings for diffusion timesteps\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# DiffusionPolicy - Combined model for policy generation\n",
    "class DiffusionPolicy:\n",
    "    def __init__(self, state_dim, action_dim, embedding_dim=64, hidden_dim=256, \n",
    "                 diffusion_steps=100, device=None):\n",
    "        \"\"\"\n",
    "        Policy that generates actions using diffusion model and decoder\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of the state space\n",
    "            action_dim: Dimension of the action space\n",
    "            embedding_dim: Dimension of the embedding space\n",
    "            hidden_dim: Dimension of hidden layers\n",
    "            diffusion_steps: Number of diffusion timesteps\n",
    "            device: Device to run computations on\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Create model components\n",
    "        self.denoising_model = ConditionalDenoisingModel(\n",
    "            embedding_dim=embedding_dim,\n",
    "            state_dim=state_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.decoder = EmbeddingDecoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            action_dim=action_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.diffusion = Diffusion(\n",
    "            embedding_dim=embedding_dim,\n",
    "            timesteps=diffusion_steps\n",
    "        )\n",
    "        \n",
    "        # Optimizers\n",
    "        self.denoising_optimizer = optim.Adam(self.denoising_model.parameters(), lr=1e-4)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=1e-4)\n",
    "        \n",
    "    def train_diffusion(self, embeddings, states, batch_size=64, epochs=100):\n",
    "        \"\"\"\n",
    "        Train the diffusion model\n",
    "        \n",
    "        Args:\n",
    "            embeddings: State-action embeddings [num_samples, embedding_dim]\n",
    "            states: Corresponding states [num_samples, state_dim]\n",
    "            batch_size: Training batch size\n",
    "            epochs: Number of training epochs\n",
    "            \n",
    "        Returns:\n",
    "            losses: List of training losses\n",
    "        \"\"\"\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.FloatTensor(embeddings).to(self.device), \n",
    "            torch.FloatTensor(states).to(self.device)\n",
    "        )\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_idx, (batch_embeddings, batch_states) in enumerate(dataloader):\n",
    "                self.denoising_optimizer.zero_grad()\n",
    "                \n",
    "                loss = self.diffusion.train_loss(self.denoising_model, batch_embeddings, batch_states)\n",
    "                loss.backward()\n",
    "                \n",
    "                self.denoising_optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "                \n",
    "        return losses\n",
    "    \n",
    "    def train_decoder(self, embeddings, actions, batch_size=64, epochs=100):\n",
    "        \"\"\"\n",
    "        Train the embedding decoder\n",
    "        \n",
    "        Args:\n",
    "            embeddings: State-action embeddings [num_samples, embedding_dim]\n",
    "            actions: Corresponding actions [num_samples, action_dim]\n",
    "            batch_size: Training batch size\n",
    "            epochs: Number of training epochs\n",
    "            \n",
    "        Returns:\n",
    "            losses: List of training losses\n",
    "        \"\"\"\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.FloatTensor(embeddings).to(self.device), \n",
    "            torch.FloatTensor(actions).to(self.device)\n",
    "        )\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_idx, (batch_embeddings, batch_actions) in enumerate(dataloader):\n",
    "                self.decoder_optimizer.zero_grad()\n",
    "                \n",
    "                predicted_actions = self.decoder(batch_embeddings)\n",
    "                loss = F.mse_loss(predicted_actions, batch_actions)\n",
    "                loss.backward()\n",
    "                \n",
    "                self.decoder_optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "                \n",
    "        return losses\n",
    "        \n",
    "    def select_action(self, state, num_samples=1, fast_sampling=False):\n",
    "        \"\"\"\n",
    "        Generate action for a given state using the diffusion policy\n",
    "        \n",
    "        Args:\n",
    "            state: Current state [state_dim]\n",
    "            num_samples: Number of action samples to generate\n",
    "            fast_sampling: If True, use fewer diffusion steps for faster inference\n",
    "            \n",
    "        Returns:\n",
    "            action: Generated action [action_dim]\n",
    "        \"\"\"\n",
    "        # Prepare state input\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            \n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "            \n",
    "        # Repeat state for multiple samples\n",
    "        state = state.repeat(num_samples, 1)\n",
    "        \n",
    "        # Generate embeddings through diffusion\n",
    "        shape = (num_samples, self.embedding_dim)\n",
    "        \n",
    "        if fast_sampling:\n",
    "            # Use fewer steps for faster inference\n",
    "            timesteps = min(10, self.diffusion.timesteps)\n",
    "            \n",
    "            # Start from pure noise\n",
    "            embeddings = torch.randn(shape).to(self.device)\n",
    "            \n",
    "            # Iteratively denoise with fewer steps\n",
    "            for i in reversed(range(0, timesteps)):\n",
    "                t = torch.full((num_samples,), i, device=self.device, dtype=torch.long)\n",
    "                t_index = i * self.diffusion.timesteps // timesteps\n",
    "                embeddings = self.diffusion.p_sample(self.denoising_model, embeddings, t, state, t_index)\n",
    "        else:\n",
    "            # Use full diffusion process\n",
    "            embeddings = self.diffusion.p_sample_loop(self.denoising_model, shape, state)\n",
    "        \n",
    "        # Decode embeddings to actions\n",
    "        with torch.no_grad():\n",
    "            actions = self.decoder(embeddings)\n",
    "            \n",
    "        if num_samples == 1:\n",
    "            return actions[0].cpu().numpy()\n",
    "        else:\n",
    "            return actions.cpu().numpy()\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model components to the specified path\"\"\"\n",
    "        torch.save({\n",
    "            'denoising_model': self.denoising_model.state_dict(),\n",
    "            'decoder': self.decoder.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the model components from the specified path\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.denoising_model.load_state_dict(checkpoint['denoising_model'])\n",
    "        self.decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def example_usage(state_dim, action_dim, num_samples=1000):\n",
    "    # Create dummy data\n",
    "    states = np.random.rand(num_samples, state_dim)\n",
    "    actions = np.random.rand(num_samples, action_dim)\n",
    "    \n",
    "    # Create embedding network (from your implementation)\n",
    "    from action_embedding_net import StateActionEmbedding\n",
    "    \n",
    "    embedding_net = StateActionEmbedding(state_dim, action_dim)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    states_tensor = torch.FloatTensor(states)\n",
    "    actions_tensor = torch.FloatTensor(actions)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_net(states_tensor, actions_tensor).numpy()\n",
    "    \n",
    "    # Create diffusion policy\n",
    "    policy = DiffusionPolicy(state_dim, action_dim)\n",
    "    \n",
    "    # Train diffusion model\n",
    "    policy.train_diffusion(embeddings, states, epochs=50)\n",
    "    \n",
    "    # Train decoder\n",
    "    policy.train_decoder(embeddings, actions, epochs=50)\n",
    "    \n",
    "    # Generate action\n",
    "    test_state = np.random.rand(state_dim)\n",
    "    action = policy.select_action(test_state)\n",
    "    \n",
    "    print(f\"Generated action: {action}\")\n",
    "    \n",
    "    # Save model\n",
    "    policy.save(\"diffusion_policy.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage(state_dim=4, action_dim=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
